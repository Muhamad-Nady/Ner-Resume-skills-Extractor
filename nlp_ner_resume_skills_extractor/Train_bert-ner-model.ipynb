{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords","metadata":{"id":"QQO06ldsDxmX","execution":{"iopub.status.busy":"2023-03-08T08:44:44.054625Z","iopub.execute_input":"2023-03-08T08:44:44.055295Z","iopub.status.idle":"2023-03-08T08:44:45.661662Z","shell.execute_reply.started":"2023-03-08T08:44:44.055258Z","shell.execute_reply":"2023-03-08T08:44:45.660564Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2023-03-06T11:44:54.227650Z","iopub.execute_input":"2023-03-06T11:44:54.228081Z","iopub.status.idle":"2023-03-06T11:48:40.830600Z","shell.execute_reply.started":"2023-03-06T11:44:54.228038Z","shell.execute_reply":"2023-03-06T11:48:40.829438Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nCollecting package metadata (current_repodata.json): done\nSolving environment: \\ \nThe environment is inconsistent, please check the package plan carefully\nThe following packages are causing the inconsistency:\n\n  - conda-forge/linux-64::imagemagick==7.1.0_62=pl5321h0dc3a92_0\n  - conda-forge/noarch::networkx==2.7=pyhd8ed1ab_0\n  - conda-forge/linux-64::cartopy==0.19.0.post1=py37h0c48da3_1\n  - conda-forge/linux-64::pillow==9.2.0=py37h850a105_2\n  - conda-forge/linux-64::openjpeg==2.5.0=h7d73246_1\n  - conda-forge/noarch::seaborn==0.12.0=hd8ed1ab_0\n  - file:///tmp/conda/linux-64::dlenv-base==1.0.20220913=py37hadde398_0\n  - conda-forge/noarch::visions==0.7.4=pyhd8ed1ab_0\n  - conda-forge/linux-64::matplotlib==3.5.3=py37h89c1867_2\n  - conda-forge/linux-64::phik==0.12.2=py37h237e563_0\n  - file:///tmp/conda/linux-64::dlenv-tf-2-9-gpu==1.0.20220913=py37hddb555a_0\n  - conda-forge/noarch::imagehash==4.3.0=pyhd8ed1ab_0\n  - conda-forge/linux-64::matplotlib-base==3.5.3=py37hf395dca_2\n  - conda-forge/noarch::pandas-profiling==3.1.0=pyhd8ed1ab_0\n  - conda-forge/noarch::missingno==0.4.2=py_1\n  - conda-forge/noarch::seaborn-base==0.12.0=pyhd8ed1ab_0\ndone\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 4.14.0\n  latest version: 23.1.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    filelock-3.9.0             |     pyhd8ed1ab_0          13 KB  conda-forge\n    gdown-4.6.4                |     pyhd8ed1ab_0          18 KB  conda-forge\n    openjpeg-2.5.0             |       hfec8fc6_2         344 KB  conda-forge\n    pillow-9.4.0               |   py37h6a678d5_0         721 KB\n    ------------------------------------------------------------\n                                           Total:         1.1 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.9.0-pyhd8ed1ab_0\n  gdown              conda-forge/noarch::gdown-4.6.4-pyhd8ed1ab_0\n\nThe following packages will be UPDATED:\n\n  openjpeg                                 2.5.0-h7d73246_1 --> 2.5.0-hfec8fc6_2\n  pillow             conda-forge::pillow-9.2.0-py37h850a10~ --> pkgs/main::pillow-9.4.0-py37h6a678d5_0\n\n\n\nDownloading and Extracting Packages\nopenjpeg-2.5.0       | 344 KB    | ##################################### | 100% \nfilelock-3.9.0       | 13 KB     | ##################################### | 100% \ngdown-4.6.4          | 18 KB     | ##################################### | 100% \npillow-9.4.0         | 721 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nRetrieving notices: ...working... done\n","output_type":"stream"}]},{"cell_type":"code","source":"## Laod model to kaggle\nurl = \"https://drive.google.com/file/d/12suEakaHP4bZUQ-MfqMEqeFG1Om820pK/view?usp=sharing\"\nurl_2 = \"https://drive.google.com/file/d/1-1J2j5ezbd_TlXC3_xbc_urQjt0EBzXe/view?usp=share_link\"\nurl_3 = \"https://drive.google.com/file/d/10KtslNMXn0sHdV9eV044okAIB-fu07SK/view?usp=share_link\"\n!gdown --id \"12suEakaHP4bZUQ-MfqMEqeFG1Om820pK\"","metadata":{"execution":{"iopub.status.busy":"2023-03-06T11:48:40.833828Z","iopub.execute_input":"2023-03-06T11:48:40.834257Z","iopub.status.idle":"2023-03-06T11:48:47.050670Z","shell.execute_reply.started":"2023-03-06T11:48:40.834211Z","shell.execute_reply":"2023-03-06T11:48:47.049510Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=12suEakaHP4bZUQ-MfqMEqeFG1Om820pK\nTo: /kaggle/working/model_scripted262.pt\n100%|█████████████████████████████████████████| 436M/436M [00:03<00:00, 118MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# bert_df_1 = pd.read_csv(\"/kaggle/input/datasettaged20k/df_tagedforbert__5.csv\")\n# bert_df_2 = pd.read_csv(\"/kaggle/input/datasettaged20k/df_tagedforbert_5_15.csv\")\n# bert_df = pd.concat([bert_df_1, bert_df_2])\nbert_df = pd.read_csv(\"/kaggle/input/20k-finalmodel/20.csv\")\nbert_df.head()","metadata":{"id":"kK9yzlHZD0OT","outputId":"b70f8795-46e6-43d2-e705-d10ccd8cb3f8","execution":{"iopub.status.busy":"2023-03-08T08:46:47.333146Z","iopub.execute_input":"2023-03-08T08:46:47.333530Z","iopub.status.idle":"2023-03-08T08:46:47.706285Z","shell.execute_reply.started":"2023-03-08T08:46:47.333476Z","shell.execute_reply":"2023-03-08T08:46:47.705214Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0  Unnamed: 0.1 sentence_id     word word_mdf tag\n0           0             0  sentence_0     good     good   O\n1           1             1  sentence_0  command  command   O\n2           2             2  sentence_0       of       of   O\n3           3             3  sentence_0     both     both   O\n4           4             4  sentence_0  english  english   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>word_mdf</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>sentence_0</td>\n      <td>good</td>\n      <td>good</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>sentence_0</td>\n      <td>command</td>\n      <td>command</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>sentence_0</td>\n      <td>of</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>sentence_0</td>\n      <td>both</td>\n      <td>both</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>sentence_0</td>\n      <td>english</td>\n      <td>english</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# some cleaning for words labels","metadata":{}},{"cell_type":"code","source":"## define column nedded for model\nbert_df = bert_df[[\"sentence_id\", \"word\", \"tag\"]]\nbert_df.reset_index(drop=True)","metadata":{"id":"vkFRFfz0-sQS","outputId":"6078cae0-54f6-484c-d4c2-c7b986bc4b07","execution":{"iopub.status.busy":"2023-03-08T08:46:54.323516Z","iopub.execute_input":"2023-03-08T08:46:54.324201Z","iopub.status.idle":"2023-03-08T08:46:54.379686Z","shell.execute_reply.started":"2023-03-08T08:46:54.324164Z","shell.execute_reply":"2023-03-08T08:46:54.378756Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           sentence_id          word tag\n0           sentence_0          good   O\n1           sentence_0       command   O\n2           sentence_0            of   O\n3           sentence_0          both   O\n4           sentence_0       english   O\n...                ...           ...  ..\n551641  sentence_19999            bb   O\n551642  sentence_19999      business   O\n551643  sentence_19999            to   O\n551644  sentence_19999      business   O\n551645  sentence_19999  applications   O\n\n[551646 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sentence_0</td>\n      <td>good</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sentence_0</td>\n      <td>command</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sentence_0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sentence_0</td>\n      <td>both</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sentence_0</td>\n      <td>english</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>551641</th>\n      <td>sentence_19999</td>\n      <td>bb</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551642</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551643</th>\n      <td>sentence_19999</td>\n      <td>to</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551644</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551645</th>\n      <td>sentence_19999</td>\n      <td>applications</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>551646 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# untag all stopwords which taged as a skill\nbert_df.reset_index(drop=True, inplace=True)\nstop_words = set(stopwords.words('english'))\ndf_stopwords = bert_df[bert_df[\"word\"].isin(stop_words)]\nindex = df_stopwords[df_stopwords[\"tag\"] != \"O\"].index\nbert_df.loc[index, \"tag\"] = \"O\"\n#bert_df.loc[index, :]\nbert_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:46:59.288408Z","iopub.execute_input":"2023-03-08T08:46:59.288867Z","iopub.status.idle":"2023-03-08T08:46:59.425956Z","shell.execute_reply.started":"2023-03-08T08:46:59.288828Z","shell.execute_reply":"2023-03-08T08:46:59.424857Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           sentence_id          word tag\n0           sentence_0          good   O\n1           sentence_0       command   O\n2           sentence_0            of   O\n3           sentence_0          both   O\n4           sentence_0       english   O\n...                ...           ...  ..\n551641  sentence_19999            bb   O\n551642  sentence_19999      business   O\n551643  sentence_19999            to   O\n551644  sentence_19999      business   O\n551645  sentence_19999  applications   O\n\n[551646 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sentence_0</td>\n      <td>good</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sentence_0</td>\n      <td>command</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sentence_0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sentence_0</td>\n      <td>both</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sentence_0</td>\n      <td>english</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>551641</th>\n      <td>sentence_19999</td>\n      <td>bb</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551642</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551643</th>\n      <td>sentence_19999</td>\n      <td>to</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551644</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551645</th>\n      <td>sentence_19999</td>\n      <td>applications</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>551646 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"bert_df.info()","metadata":{"id":"kIr_amtq_YNx","outputId":"f0a53ab5-86f6-4531-a8f4-7f8259e9444a","execution":{"iopub.status.busy":"2023-03-08T08:47:02.119915Z","iopub.execute_input":"2023-03-08T08:47:02.120266Z","iopub.status.idle":"2023-03-08T08:47:02.194253Z","shell.execute_reply.started":"2023-03-08T08:47:02.120236Z","shell.execute_reply":"2023-03-08T08:47:02.193287Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 551646 entries, 0 to 551645\nData columns (total 3 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   sentence_id  551646 non-null  object\n 1   word         551642 non-null  object\n 2   tag          551646 non-null  object\ndtypes: object(3)\nmemory usage: 12.6+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_df[bert_df[\"word\"].isnull()]","metadata":{"id":"OFWx4ZNR_g_Q","outputId":"a1105920-c92d-4f13-b62b-2381b73be320","execution":{"iopub.status.busy":"2023-03-08T08:47:02.965287Z","iopub.execute_input":"2023-03-08T08:47:02.965650Z","iopub.status.idle":"2023-03-08T08:47:02.997104Z","shell.execute_reply.started":"2023-03-08T08:47:02.965620Z","shell.execute_reply":"2023-03-08T08:47:02.996000Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          sentence_id word tag\n67468   sentence_2621  NaN   O\n67469   sentence_2621  NaN   O\n192879  sentence_7187  NaN   O\n192880  sentence_7187  NaN   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>67468</th>\n      <td>sentence_2621</td>\n      <td>NaN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>67469</th>\n      <td>sentence_2621</td>\n      <td>NaN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>192879</th>\n      <td>sentence_7187</td>\n      <td>NaN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>192880</th>\n      <td>sentence_7187</td>\n      <td>NaN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"bert_df.iloc[1254:,:]","metadata":{"id":"PbWE71YK_6V4","outputId":"0ebcc611-3d38-44d5-d63e-4b6de65a1d22","execution":{"iopub.status.busy":"2023-03-08T08:47:05.407354Z","iopub.execute_input":"2023-03-08T08:47:05.408015Z","iopub.status.idle":"2023-03-08T08:47:05.420660Z","shell.execute_reply.started":"2023-03-08T08:47:05.407980Z","shell.execute_reply":"2023-03-08T08:47:05.419553Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"           sentence_id            word tag\n1254       sentence_62          assume   O\n1255       sentence_62           every   O\n1256       sentence_62      increasing   O\n1257       sentence_62  responsibility   O\n1258       sentence_62              in   O\n...                ...             ...  ..\n551641  sentence_19999              bb   O\n551642  sentence_19999        business   O\n551643  sentence_19999              to   O\n551644  sentence_19999        business   O\n551645  sentence_19999    applications   O\n\n[550392 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1254</th>\n      <td>sentence_62</td>\n      <td>assume</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1255</th>\n      <td>sentence_62</td>\n      <td>every</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1256</th>\n      <td>sentence_62</td>\n      <td>increasing</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1257</th>\n      <td>sentence_62</td>\n      <td>responsibility</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1258</th>\n      <td>sentence_62</td>\n      <td>in</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>551641</th>\n      <td>sentence_19999</td>\n      <td>bb</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551642</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551643</th>\n      <td>sentence_19999</td>\n      <td>to</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551644</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>551645</th>\n      <td>sentence_19999</td>\n      <td>applications</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>550392 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"bert_df[\"tag\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:47:11.047663Z","iopub.execute_input":"2023-03-08T08:47:11.048046Z","iopub.status.idle":"2023-03-08T08:47:11.077617Z","shell.execute_reply.started":"2023-03-08T08:47:11.048014Z","shell.execute_reply":"2023-03-08T08:47:11.076404Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"O          482814\nB-SKILL     54879\nI-SKILL     12657\nO-SKILL      1296\nName: tag, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"!pip install transformers seqeval[gpu]","metadata":{"id":"hZvBFGHjD7HB","outputId":"c3d51601-483f-49f2-d932-dae7141d715b","execution":{"iopub.status.busy":"2023-03-08T08:47:13.960251Z","iopub.execute_input":"2023-03-08T08:47:13.960624Z","iopub.status.idle":"2023-03-08T08:47:30.129840Z","shell.execute_reply.started":"2023-03-08T08:47:13.960592Z","shell.execute_reply":"2023-03-08T08:47:30.128521Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting seqeval[gpu]\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval[gpu]) (1.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=6415184eaff61edb25e883a35c13df323ed20e8b6f80f735222d9cbad81572b4\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification","metadata":{"id":"TqbomZTyEMRp","execution":{"iopub.status.busy":"2023-03-08T08:48:12.543436Z","iopub.execute_input":"2023-03-08T08:48:12.543909Z","iopub.status.idle":"2023-03-08T08:48:12.551728Z","shell.execute_reply.started":"2023-03-08T08:48:12.543868Z","shell.execute_reply":"2023-03-08T08:48:12.549464Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"id":"g1PuFA-cERbx","outputId":"7bbe77e2-694b-4e9d-c7cd-83072c08424a","execution":{"iopub.status.busy":"2023-03-08T08:48:14.407321Z","iopub.execute_input":"2023-03-08T08:48:14.407934Z","iopub.status.idle":"2023-03-08T08:48:14.414207Z","shell.execute_reply.started":"2023-03-08T08:48:14.407897Z","shell.execute_reply":"2023-03-08T08:48:14.413025Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\nbert_df = bert_df.fillna(method='ffill')\nbert_df.head()","metadata":{"id":"iJ_gc87-EgpI","outputId":"8404be42-7640-4482-ed53-339e23eb5f23","execution":{"iopub.status.busy":"2023-03-08T08:48:15.914507Z","iopub.execute_input":"2023-03-08T08:48:15.915533Z","iopub.status.idle":"2023-03-08T08:48:16.023680Z","shell.execute_reply.started":"2023-03-08T08:48:15.915469Z","shell.execute_reply":"2023-03-08T08:48:16.022465Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"  sentence_id     word tag\n0  sentence_0     good   O\n1  sentence_0  command   O\n2  sentence_0       of   O\n3  sentence_0     both   O\n4  sentence_0  english   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sentence_0</td>\n      <td>good</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sentence_0</td>\n      <td>command</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sentence_0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sentence_0</td>\n      <td>both</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sentence_0</td>\n      <td>english</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# let's create a new column called \"sentence\" which groups the words by sentence \ndata = bert_df\ndata.rename(columns={\"sentence_id\": \"Sentence #\", \"word\":\"Word\", \"tag\":\"Tag\"}, inplace=True)\ndata['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n# let's also create a new column called \"word_labels\" which groups the tags by sentence \ndata['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\ndata.tail()","metadata":{"id":"lye-HwjPEqeb","outputId":"5849f7b8-67ab-4154-ec9d-beeaee23da33","execution":{"iopub.status.busy":"2023-03-08T08:48:17.620298Z","iopub.execute_input":"2023-03-08T08:48:17.620681Z","iopub.status.idle":"2023-03-08T08:48:21.883263Z","shell.execute_reply.started":"2023-03-08T08:48:17.620647Z","shell.execute_reply":"2023-03-08T08:48:21.882300Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"            Sentence #          Word Tag  \\\n551641  sentence_19999            bb   O   \n551642  sentence_19999      business   O   \n551643  sentence_19999            to   O   \n551644  sentence_19999      business   O   \n551645  sentence_19999  applications   O   \n\n                                                 sentence  \\\n551641  complete understanding of software development...   \n551642  complete understanding of software development...   \n551643  complete understanding of software development...   \n551644  complete understanding of software development...   \n551645  complete understanding of software development...   \n\n                                              word_labels  \n551641  O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...  \n551642  O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...  \n551643  O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...  \n551644  O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...  \n551645  O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>Tag</th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>551641</th>\n      <td>sentence_19999</td>\n      <td>bb</td>\n      <td>O</td>\n      <td>complete understanding of software development...</td>\n      <td>O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n    </tr>\n    <tr>\n      <th>551642</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n      <td>complete understanding of software development...</td>\n      <td>O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n    </tr>\n    <tr>\n      <th>551643</th>\n      <td>sentence_19999</td>\n      <td>to</td>\n      <td>O</td>\n      <td>complete understanding of software development...</td>\n      <td>O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n    </tr>\n    <tr>\n      <th>551644</th>\n      <td>sentence_19999</td>\n      <td>business</td>\n      <td>O</td>\n      <td>complete understanding of software development...</td>\n      <td>O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n    </tr>\n    <tr>\n      <th>551645</th>\n      <td>sentence_19999</td>\n      <td>applications</td>\n      <td>O</td>\n      <td>complete understanding of software development...</td>\n      <td>O,O,O,B-SKILL,I-SKILL,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"label2id = {k: v for v, k in enumerate(data.Tag.unique())}\nid2label = {v: k for v, k in enumerate(data.Tag.unique())}\nlabel2id","metadata":{"id":"XXayxEo0Hd1n","outputId":"9ed2a26a-9d3f-41fd-c03f-b7ec201916ab","execution":{"iopub.status.busy":"2023-03-08T08:48:24.123006Z","iopub.execute_input":"2023-03-08T08:48:24.123354Z","iopub.status.idle":"2023-03-08T08:48:24.191207Z","shell.execute_reply.started":"2023-03-08T08:48:24.123325Z","shell.execute_reply":"2023-03-08T08:48:24.190193Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'O': 0, 'B-SKILL': 1, 'I-SKILL': 2, 'O-SKILL': 3}"},"metadata":{}}]},{"cell_type":"code","source":"data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\ndata.head()","metadata":{"id":"yfAJt-L9F91e","outputId":"1e8f21e6-42ce-4b81-8f73-d66719086be1","execution":{"iopub.status.busy":"2023-03-08T08:48:25.782581Z","iopub.execute_input":"2023-03-08T08:48:25.782948Z","iopub.status.idle":"2023-03-08T08:48:26.364391Z","shell.execute_reply.started":"2023-03-08T08:48:25.782917Z","shell.execute_reply":"2023-03-08T08:48:26.363430Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                            sentence  \\\n0    good command of both english & arabic languages   \n1                                 must be a graduate   \n2  eriences in a development role for microsoft d...   \n3  solid understanding of programming languages c...   \n4           solid understating of data base concepts   \n\n                   word_labels  \n0              O,O,O,O,O,O,O,O  \n1                      O,O,O,O  \n2  O,O,O,O,O,O,B-SKILL,I-SKILL  \n3              O,O,O,O,O,O,O,O  \n4                  O,O,O,O,O,O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>word_labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good command of both english &amp; arabic languages</td>\n      <td>O,O,O,O,O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>must be a graduate</td>\n      <td>O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eriences in a development role for microsoft d...</td>\n      <td>O,O,O,O,O,O,B-SKILL,I-SKILL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>solid understanding of programming languages c...</td>\n      <td>O,O,O,O,O,O,O,O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>solid understating of data base concepts</td>\n      <td>O,O,O,O,O,O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-08T08:48:27.182048Z","iopub.execute_input":"2023-03-08T08:48:27.182404Z","iopub.status.idle":"2023-03-08T08:48:27.190279Z","shell.execute_reply.started":"2023-03-08T08:48:27.182373Z","shell.execute_reply":"2023-03-08T08:48:27.188922Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(17354, 2)"},"metadata":{}}]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 16\nEPOCHS = 10\nLEARNING_RATE = 1e-05\nMAX_GRAD_NORM = 10\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"id":"Z-rmWoiqGS6l","execution":{"iopub.status.busy":"2023-03-08T08:48:28.778366Z","iopub.execute_input":"2023-03-08T08:48:28.779250Z","iopub.status.idle":"2023-03-08T08:48:30.279465Z","shell.execute_reply.started":"2023-03-08T08:48:28.779214Z","shell.execute_reply":"2023-03-08T08:48:30.278482Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a597cd333e47bda98f6ef126068abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c05ab2f3ca34d59936598ceb3e1545f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b0ab3df0c3404b94da1da29f005827"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n\n    sentence = sentence.strip()\n\n    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence.extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n        labels.extend([label] * n_subwords)\n\n    return tokenized_sentence, labels","metadata":{"id":"c4clCKbyGY0N","execution":{"iopub.status.busy":"2023-03-08T08:48:33.899417Z","iopub.execute_input":"2023-03-08T08:48:33.900099Z","iopub.status.idle":"2023-03-08T08:48:33.906538Z","shell.execute_reply.started":"2023-03-08T08:48:33.900063Z","shell.execute_reply":"2023-03-08T08:48:33.905478Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        # step 1: tokenize (and adapt corresponding labels)\n        sentence = self.data.sentence[index]  \n        word_labels = self.data.word_labels[index]  \n        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n        \n        # step 2: add special tokens (and corresponding labels)\n        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n        labels.insert(0, \"O\") # add outside label for [CLS] token\n        labels.insert(-1, \"O\") # add outside label for [SEP] token\n\n        # step 3: truncating/padding\n        maxlen = self.max_len\n\n        if (len(tokenized_sentence) > maxlen):\n          # truncate\n          tokenized_sentence = tokenized_sentence[:maxlen]\n          labels = labels[:maxlen]\n        else:\n          # pad\n          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n\n        # step 4: obtain the attention mask\n        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n        \n        # step 5: convert tokens to input ids\n        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n\n        label_ids = [label2id[label] for label in labels]\n        # the following line is deprecated\n        #label_ids = [label if label != 0 else -100 for label in label_ids]\n        \n        return {\n              'ids': torch.tensor(ids, dtype=torch.long),\n              'mask': torch.tensor(attn_mask, dtype=torch.long),\n              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n              'targets': torch.tensor(label_ids, dtype=torch.long)\n        } \n    \n    def __len__(self):\n        return self.len","metadata":{"id":"UxxAGRaWGdnN","execution":{"iopub.status.busy":"2023-03-08T08:48:34.257318Z","iopub.execute_input":"2023-03-08T08:48:34.258020Z","iopub.status.idle":"2023-03-08T08:48:34.269381Z","shell.execute_reply.started":"2023-03-08T08:48:34.257983Z","shell.execute_reply":"2023-03-08T08:48:34.267994Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_size = 0.8\ntrain_dataset = data.sample(frac=train_size,random_state=200)\ntest_dataset = data.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = dataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = dataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"id":"EfM0ZuGCGkzm","outputId":"5eb64158-e0f4-4844-da7e-ba78b00f97c1","execution":{"iopub.status.busy":"2023-03-08T08:48:35.717103Z","iopub.execute_input":"2023-03-08T08:48:35.717464Z","iopub.status.idle":"2023-03-08T08:48:35.737727Z","shell.execute_reply.started":"2023-03-08T08:48:35.717433Z","shell.execute_reply":"2023-03-08T08:48:35.736709Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"FULL Dataset: (17354, 2)\nTRAIN Dataset: (13883, 2)\nTEST Dataset: (3471, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"training_set[0]","metadata":{"id":"r0OMfrvVGnYt","outputId":"dcdec84d-18f0-46c0-8fa6-486b5fc3cf13","execution":{"iopub.status.busy":"2023-03-08T08:48:36.114318Z","iopub.execute_input":"2023-03-08T08:48:36.115009Z","iopub.status.idle":"2023-03-08T08:48:36.168697Z","shell.execute_reply.started":"2023-03-08T08:48:36.114972Z","shell.execute_reply":"2023-03-08T08:48:36.167762Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'ids': tensor([  101,  2478,  1996, 22889,  2050,  2565,  2000,  3113,  1996, 15117,\n          1998, 12422,  1996, 10813,  8122,  1998, 10640,   102,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]),\n 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0])}"},"metadata":{}}]},{"cell_type":"code","source":"# print the first 30 tokens and corresponding labels\nfor token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n  print('{0:10}  {1}'.format(token, id2label[label.item()]))","metadata":{"id":"quFqaTmtGps1","outputId":"2c8ecfb5-57da-4eff-e602-1d11fbc8d796","execution":{"iopub.status.busy":"2023-03-08T08:48:37.628007Z","iopub.execute_input":"2023-03-08T08:48:37.628361Z","iopub.status.idle":"2023-03-08T08:48:37.645329Z","shell.execute_reply.started":"2023-03-08T08:48:37.628331Z","shell.execute_reply":"2023-03-08T08:48:37.644037Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[CLS]       O\nusing       O\nthe         O\nsl          O\n##a         O\nprogram     O\nto          O\nmeet        O\nthe         O\ndeadline    O\nand         O\nunsure      O\nthe         O\ndeployment  B-SKILL\nefficiency  O\nand         O\naccuracy    O\n[SEP]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n[PAD]       O\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"id":"yVS8_TJGJ5Tp","execution":{"iopub.status.busy":"2023-03-08T08:48:39.267644Z","iopub.execute_input":"2023-03-08T08:48:39.268052Z","iopub.status.idle":"2023-03-08T08:48:39.275893Z","shell.execute_reply.started":"2023-03-08T08:48:39.268018Z","shell.execute_reply":"2023-03-08T08:48:39.274835Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n                                                    num_labels=len(id2label),\n                                                    id2label=id2label,\n                                                    label2id=label2id)\n#model = torch.load('/kaggle/working/model_scripted262.pt')\nmodel.to(device)","metadata":{"id":"CRDSU8fgJ9zW","outputId":"18be7aea-90a5-4f1f-acfe-f862f9daacad","execution":{"iopub.status.busy":"2023-03-08T08:48:42.948384Z","iopub.execute_input":"2023-03-08T08:48:42.949065Z","iopub.status.idle":"2023-03-08T08:48:55.790185Z","shell.execute_reply.started":"2023-03-08T08:48:42.949030Z","shell.execute_reply":"2023-03-08T08:48:55.787558Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a20753365b64f74891ec6193be3d69c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"ids = training_set[0][\"ids\"].unsqueeze(0)\nmask = training_set[0][\"mask\"].unsqueeze(0)\ntargets = training_set[0][\"targets\"].unsqueeze(0)\nids = ids.to(device)\nmask = mask.to(device)\ntargets = targets.to(device)\noutputs = model(input_ids=ids, attention_mask=mask, labels=targets)\ninitial_loss = outputs[0]\ninitial_loss","metadata":{"id":"wlNC8pg9J_3G","outputId":"c0f611ab-1a70-47bd-a0d2-496fef39a5e9","execution":{"iopub.status.busy":"2023-03-08T08:48:55.793673Z","iopub.execute_input":"2023-03-08T08:48:55.793991Z","iopub.status.idle":"2023-03-08T08:48:58.335434Z","shell.execute_reply.started":"2023-03-08T08:48:55.793956Z","shell.execute_reply":"2023-03-08T08:48:58.334444Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"tensor(1.2139, device='cuda:0', grad_fn=<NllLossBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"tr_logits = outputs[1]\ntr_logits.shape","metadata":{"id":"juumdf6TKGac","outputId":"0f309417-7808-4512-a333-0aa4a5f106bd","execution":{"iopub.status.busy":"2023-03-08T08:48:58.336884Z","iopub.execute_input":"2023-03-08T08:48:58.337358Z","iopub.status.idle":"2023-03-08T08:48:58.344358Z","shell.execute_reply.started":"2023-03-08T08:48:58.337318Z","shell.execute_reply":"2023-03-08T08:48:58.343358Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128, 4])"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"id":"z-aWjpknKeDB","execution":{"iopub.status.busy":"2023-03-08T08:48:58.347282Z","iopub.execute_input":"2023-03-08T08:48:58.347981Z","iopub.status.idle":"2023-03-08T08:48:58.355343Z","shell.execute_reply.started":"2023-03-08T08:48:58.347935Z","shell.execute_reply":"2023-03-08T08:48:58.354294Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertConfig\n\noutput_model = '/content/drive/MyDrive/Certifications/model_xlnet_mid.pth'\ndef save(model, optimizer):\n    # save\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }, output_model)","metadata":{"id":"lzaj2d7S9JH-","execution":{"iopub.status.busy":"2023-03-08T08:48:58.356928Z","iopub.execute_input":"2023-03-08T08:48:58.357312Z","iopub.status.idle":"2023-03-08T08:48:58.366694Z","shell.execute_reply.started":"2023-03-08T08:48:58.357278Z","shell.execute_reply":"2023-03-08T08:48:58.365667Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the bert model\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['ids'].to(device, dtype = torch.long)\n        mask = batch['mask'].to(device, dtype = torch.long)\n        targets = batch['targets'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n        loss, tr_logits = outputs.loss, outputs.logits\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += targets.size(0)\n        \n        if idx % 100==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n        targets = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_preds.extend(predictions)\n        tr_labels.extend(targets)\n        \n        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")\n    #save(model, optimizer)\n    torch.save(model, '/kaggle/working//model_scripted6323.pt')","metadata":{"id":"wULfPJ1hKJRU","execution":{"iopub.status.busy":"2023-03-08T08:48:58.368345Z","iopub.execute_input":"2023-03-08T08:48:58.368928Z","iopub.status.idle":"2023-03-08T08:48:58.382600Z","shell.execute_reply.started":"2023-03-08T08:48:58.368887Z","shell.execute_reply":"2023-03-08T08:48:58.381393Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"%%time\nfor epoch in range(20):\n    print(f\"Training epoch: {epoch + 1}\")\n    train(epoch)","metadata":{"id":"BvxjaOiZKNOs","outputId":"119a53ba-faaf-4908-d8d5-58ac119067de","execution":{"iopub.status.busy":"2023-03-08T08:49:15.632300Z","iopub.execute_input":"2023-03-08T08:49:15.632671Z","iopub.status.idle":"2023-03-08T10:37:56.395219Z","shell.execute_reply.started":"2023-03-08T08:49:15.632639Z","shell.execute_reply":"2023-03-08T10:37:56.394203Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Training epoch: 1\nTraining loss per 100 training steps: 1.2709558010101318\nTraining loss per 100 training steps: 0.18787004145802838\nTraining loss per 100 training steps: 0.1252772487069837\nTraining loss per 100 training steps: 0.09842278302780219\nTraining loss per 100 training steps: 0.08255397079470672\nTraining loss epoch: 0.07855395925304239\nTraining accuracy epoch: 0.9322280468965433\nTraining epoch: 2\nTraining loss per 100 training steps: 0.02533300593495369\nTraining loss per 100 training steps: 0.028124927709081975\nTraining loss per 100 training steps: 0.025721905780817147\nTraining loss per 100 training steps: 0.024103198178805584\nTraining loss per 100 training steps: 0.02281806006809311\nTraining loss epoch: 0.022610361530115048\nTraining accuracy epoch: 0.9745756796640919\nTraining epoch: 3\nTraining loss per 100 training steps: 0.021969251334667206\nTraining loss per 100 training steps: 0.015002444084493978\nTraining loss per 100 training steps: 0.014912371951113664\nTraining loss per 100 training steps: 0.014409449167295894\nTraining loss per 100 training steps: 0.01397902886743223\nTraining loss epoch: 0.01398487012266838\nTraining accuracy epoch: 0.9851078225889922\nTraining epoch: 4\nTraining loss per 100 training steps: 0.018019182607531548\nTraining loss per 100 training steps: 0.011435267527006788\nTraining loss per 100 training steps: 0.010949745284282227\nTraining loss per 100 training steps: 0.010342845221381151\nTraining loss per 100 training steps: 0.01008761528544964\nTraining loss epoch: 0.010010002802441558\nTraining accuracy epoch: 0.9892828024866991\nTraining epoch: 5\nTraining loss per 100 training steps: 0.008984512649476528\nTraining loss per 100 training steps: 0.007334231152337524\nTraining loss per 100 training steps: 0.007628124512486456\nTraining loss per 100 training steps: 0.007498161878283646\nTraining loss per 100 training steps: 0.00726255203787112\nTraining loss epoch: 0.007220988768214051\nTraining accuracy epoch: 0.9925456611475609\nTraining epoch: 6\nTraining loss per 100 training steps: 0.004893045872449875\nTraining loss per 100 training steps: 0.005536464736266707\nTraining loss per 100 training steps: 0.005561921498518141\nTraining loss per 100 training steps: 0.005623414664625181\nTraining loss per 100 training steps: 0.005457678091819131\nTraining loss epoch: 0.005445989494722697\nTraining accuracy epoch: 0.9942649797190878\nTraining epoch: 7\nTraining loss per 100 training steps: 0.0013022430939599872\nTraining loss per 100 training steps: 0.004059230963789425\nTraining loss per 100 training steps: 0.004146742155492788\nTraining loss per 100 training steps: 0.004096029033927724\nTraining loss per 100 training steps: 0.0041512381199786804\nTraining loss epoch: 0.0041722606933001895\nTraining accuracy epoch: 0.9956779644822523\nTraining epoch: 8\nTraining loss per 100 training steps: 0.003798958845436573\nTraining loss per 100 training steps: 0.0034311805237407364\nTraining loss per 100 training steps: 0.0033347718067240173\nTraining loss per 100 training steps: 0.0034555373657293668\nTraining loss per 100 training steps: 0.0033853922171798068\nTraining loss epoch: 0.0033781786931679726\nTraining accuracy epoch: 0.9963723737741952\nTraining epoch: 9\nTraining loss per 100 training steps: 0.0038091344758868217\nTraining loss per 100 training steps: 0.0023885245396326857\nTraining loss per 100 training steps: 0.002440229009904097\nTraining loss per 100 training steps: 0.0026135798947857784\nTraining loss per 100 training steps: 0.002632253971185995\nTraining loss epoch: 0.0026573401440309428\nTraining accuracy epoch: 0.9971429167133354\nTraining epoch: 10\nTraining loss per 100 training steps: 0.0035007502883672714\nTraining loss per 100 training steps: 0.002165258467735695\nTraining loss per 100 training steps: 0.0021732054182081553\nTraining loss per 100 training steps: 0.0020921524450011964\nTraining loss per 100 training steps: 0.0021452657926518195\nTraining loss epoch: 0.0021621201079808743\nTraining accuracy epoch: 0.9976206949047158\nTraining epoch: 11\nTraining loss per 100 training steps: 0.002268043113872409\nTraining loss per 100 training steps: 0.0017369490433518896\nTraining loss per 100 training steps: 0.001844244511752848\nTraining loss per 100 training steps: 0.0017741285217838885\nTraining loss per 100 training steps: 0.0018242832214347704\nTraining loss epoch: 0.0018106584419186006\nTraining accuracy epoch: 0.9980223725264397\nTraining epoch: 12\nTraining loss per 100 training steps: 0.0015619458863511682\nTraining loss per 100 training steps: 0.0015700436672679895\nTraining loss per 100 training steps: 0.001429611605601339\nTraining loss per 100 training steps: 0.0014011745030600717\nTraining loss per 100 training steps: 0.0013548026471837142\nTraining loss epoch: 0.0013620015856402729\nTraining accuracy epoch: 0.9985283190512462\nTraining epoch: 13\nTraining loss per 100 training steps: 0.0008857648936100304\nTraining loss per 100 training steps: 0.0010582772899784452\nTraining loss per 100 training steps: 0.0010512582887518937\nTraining loss per 100 training steps: 0.001054512496461954\nTraining loss per 100 training steps: 0.0011196310754297556\nTraining loss epoch: 0.0011256805403043531\nTraining accuracy epoch: 0.9987353044324079\nTraining epoch: 14\nTraining loss per 100 training steps: 0.00010542177187744528\nTraining loss per 100 training steps: 0.0010055609499530793\nTraining loss per 100 training steps: 0.001078527618730406\nTraining loss per 100 training steps: 0.0010764349927223423\nTraining loss per 100 training steps: 0.001087251166618421\nTraining loss epoch: 0.0010424457052176709\nTraining accuracy epoch: 0.9988011355513597\nTraining epoch: 15\nTraining loss per 100 training steps: 0.00013086575199849904\nTraining loss per 100 training steps: 0.0007058058875886087\nTraining loss per 100 training steps: 0.0008424549396385831\nTraining loss per 100 training steps: 0.0008235591169515551\nTraining loss per 100 training steps: 0.0008230777042209545\nTraining loss epoch: 0.0008260412669439118\nTraining accuracy epoch: 0.9990954395886353\nTraining epoch: 16\nTraining loss per 100 training steps: 0.0014946374576538801\nTraining loss per 100 training steps: 0.0005413467446811806\nTraining loss per 100 training steps: 0.0005872713299145267\nTraining loss per 100 training steps: 0.0006061840450937088\nTraining loss per 100 training steps: 0.0006169200915744687\nTraining loss epoch: 0.0006378020204008586\nTraining accuracy epoch: 0.9992599616156431\nTraining epoch: 17\nTraining loss per 100 training steps: 0.00021742298849858344\nTraining loss per 100 training steps: 0.0005891354686526136\nTraining loss per 100 training steps: 0.0006692207637330574\nTraining loss per 100 training steps: 0.0006225467189798651\nTraining loss per 100 training steps: 0.0006557985257165825\nTraining loss epoch: 0.0006564550551952736\nTraining accuracy epoch: 0.999287164945916\nTraining epoch: 18\nTraining loss per 100 training steps: 0.00016044461517594755\nTraining loss per 100 training steps: 0.0005004496124816832\nTraining loss per 100 training steps: 0.0006001578840278396\nTraining loss per 100 training steps: 0.0005787087060483362\nTraining loss per 100 training steps: 0.000565719136904858\nTraining loss epoch: 0.000567939296781358\nTraining accuracy epoch: 0.9993546270825749\nTraining epoch: 19\nTraining loss per 100 training steps: 0.0001706157490843907\nTraining loss per 100 training steps: 0.0003759399279251828\nTraining loss per 100 training steps: 0.0004281561664379183\nTraining loss per 100 training steps: 0.00041429762148804597\nTraining loss per 100 training steps: 0.00041628621503624524\nTraining loss epoch: 0.00044097684132148355\nTraining accuracy epoch: 0.9995148867910154\nTraining epoch: 20\nTraining loss per 100 training steps: 6.888120697112754e-05\nTraining loss per 100 training steps: 0.0004005977637524945\nTraining loss per 100 training steps: 0.0004284142030168305\nTraining loss per 100 training steps: 0.00039656186477123354\nTraining loss per 100 training steps: 0.00045013732684546403\nTraining loss epoch: 0.0004573762152834663\nTraining accuracy epoch: 0.9994986930793969\nCPU times: user 1h 47min 31s, sys: 31.7 s, total: 1h 48min 3s\nWall time: 1h 48min 40s\n","output_type":"stream"}]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['ids'].to(device, dtype = torch.long)\n            mask = batch['mask'].to(device, dtype = torch.long)\n            targets = batch['targets'].to(device, dtype = torch.long)\n            \n            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n            loss, eval_logits = outputs.loss, outputs.logits\n            \n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += targets.size(0)\n        \n            if idx % 100==0:\n                loss_step = eval_loss/nb_eval_steps\n                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n            targets = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            \n            eval_labels.extend(targets)\n            eval_preds.extend(predictions)\n            \n            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n    \n    #print(eval_labels)\n    #print(eval_preds)\n\n    labels = [id2label[id.item()] for id in eval_labels]\n    predictions = [id2label[id.item()] for id in eval_preds]\n\n    #print(labels)\n    #print(predictions)\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions","metadata":{"id":"aV8ywaFuKQns","execution":{"iopub.status.busy":"2023-03-08T10:37:56.397950Z","iopub.execute_input":"2023-03-08T10:37:56.398627Z","iopub.status.idle":"2023-03-08T10:37:56.411639Z","shell.execute_reply.started":"2023-03-08T10:37:56.398582Z","shell.execute_reply":"2023-03-08T10:37:56.410687Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"labels, predictions = valid(model, testing_loader)","metadata":{"id":"4HhFmQ7GUX0c","outputId":"7331e799-268e-4645-ca16-8d9d3445be46","execution":{"iopub.status.busy":"2023-03-08T10:37:56.414857Z","iopub.execute_input":"2023-03-08T10:37:56.415119Z","iopub.status.idle":"2023-03-08T10:38:30.392079Z","shell.execute_reply.started":"2023-03-08T10:37:56.415095Z","shell.execute_reply":"2023-03-08T10:38:30.391015Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Validation loss per 100 evaluation steps: 0.0004924497334286571\nValidation loss per 100 evaluation steps: 0.007858470556345877\nValidation loss per 100 evaluation steps: 0.009057898696523604\nValidation Loss: 0.008946437419837007\nValidation Accuracy: 0.9936946865323996\n","output_type":"stream"}]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\nprint(classification_report([labels], [predictions]))","metadata":{"id":"v5uUama1UaT8","outputId":"4fc4a33f-802f-48c4-de4f-9c1c6045a2db","execution":{"iopub.status.busy":"2023-03-08T10:38:30.395144Z","iopub.execute_input":"2023-03-08T10:38:30.396093Z","iopub.status.idle":"2023-03-08T10:38:32.216085Z","shell.execute_reply.started":"2023-03-08T10:38:30.396055Z","shell.execute_reply":"2023-03-08T10:38:32.214833Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O-SKILL seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       SKILL       0.97      0.98      0.97     10498\n\n   micro avg       0.97      0.98      0.97     10498\n   macro avg       0.97      0.98      0.97     10498\nweighted avg       0.97      0.98      0.97     10498\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inferance","metadata":{}},{"cell_type":"code","source":"text = \"\"\"Ahmed Ramzy Fathy Saleh                                                   Details:\nMachine learning engineer on cloud                                                       phone: 01090668772\n                                                                                        Date of birth: 8/9/200\n                                                                                                                Military service: executed\n                                                                                                   Mail:ahmed.ramzy.2000.22@gmail.com\nObjectives:\nTo get a challenging career that can help me improve my skills & I could express myself through it, to gain more information & more skills in my professional life so I could be useful more & more for a Such a successful organization.\nEmployment history:\nMachine learning engineer on the cloud at NTI, Nasr city   \nNov 2022-Jan 2023. \nEducation:\nGraduated from October 6 university faculty of computer science Department of information system (June 2022) \n• Cumulative Grade: very good \n• Graduated project: online pizza ordering system \nTraining &Courses: \n• Introduction to the network (CCNA)                                                  - machine learning on cloud\t\n• Egypt fwd. WEB Development Challenger Track (Aug 2021)          -Natural Language Processing \n• Egypt fwd. data analytics                                                                    -Freelancing Skills (12hrs)\n• Linked in Certificate of data analytics on (July 03, 2022)                  - Project Management (36hrs)\n• Certification of (HCIA, 5G) ( Jan 07, 2024) \t                         \t       -Soft Skills (30hrs)\n• HCIA big data \n• Fundamental of data base Mahartech.gov \t        \n• Oracle SQL database                                                                  \n•Introduction to Data Center (36hr)                                              -\t\n•Programming with Python (PCAP):\n•Cloud Computing Concepts\n-AWS Core Services.\n-AWS Security.\n-AWS Architecting.\n-AWS Cloud Economics.\nSkills: \n• MS Office (Word, Powerpoint, excel).\n•Fast learner\n• Adaptability\n•Ability to work on a team\n•Excellent communication skills\n•creative thinking\nReferences\nUpon your request either in soft or hard copy for graduation certificate, military, courses, and other certificates\n\n\n\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:38:32.217931Z","iopub.execute_input":"2023-03-08T10:38:32.218626Z","iopub.status.idle":"2023-03-08T10:38:32.226768Z","shell.execute_reply.started":"2023-03-08T10:38:32.218566Z","shell.execute_reply":"2023-03-08T10:38:32.225665Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!pip install clean-text\n!pip uninstall emoji -y\n!pip install emoji==1.7\n!pip install clean-text[gpl]","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:38:32.228187Z","iopub.execute_input":"2023-03-08T10:38:32.228688Z","iopub.status.idle":"2023-03-08T10:39:06.929999Z","shell.execute_reply.started":"2023-03-08T10:38:32.228645Z","shell.execute_reply":"2023-03-08T10:39:06.928770Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nCollecting clean-text\n  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nCollecting emoji<2.0.0,>=1.0.0\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy<7.0,>=6.0\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=ca7228140c08d023bc6166754d8fb406133707e6fff8a6448c176248998d951a\n  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\nSuccessfully built emoji\nInstalling collected packages: emoji, ftfy, clean-text\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.2.0\n    Uninstalling emoji-2.2.0:\n      Successfully uninstalled emoji-2.2.0\nSuccessfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nFound existing installation: emoji 1.7.0\nUninstalling emoji-1.7.0:\n  Successfully uninstalled emoji-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nCollecting emoji==1.7\n  Using cached emoji-1.7.0-py3-none-any.whl\nInstalling collected packages: emoji\nSuccessfully installed emoji-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: clean-text[gpl] in /opt/conda/lib/python3.7/site-packages (0.6.0)\nRequirement already satisfied: emoji<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from clean-text[gpl]) (1.7.0)\nRequirement already satisfied: ftfy<7.0,>=6.0 in /opt/conda/lib/python3.7/site-packages (from clean-text[gpl]) (6.1.1)\nRequirement already satisfied: unidecode<2.0.0,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from clean-text[gpl]) (1.3.6)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy<7.0,>=6.0->clean-text[gpl]) (0.2.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"## cleaning corpus with cleantetx fun\nfrom cleantext import clean\nnew_txt = clean(text, lower=True,\n    fix_unicode=True,\n    to_ascii=True,\n    normalize_whitespace=False,\n    no_line_breaks=True,\n    strip_lines=True,\n    keep_two_line_breaks=False,\n    no_urls=True,\n    no_emails=False,\n    no_phone_numbers=True,\n    no_numbers=False,\n    no_digits=False,\n    no_currency_symbols=True,\n    no_punct=False,\n    no_emoji=True,\n    replace_with_url=\"<URL>\",\n    replace_with_email=\"<EMAIL>\",\n    replace_with_phone_number=\"<PHONE>\",\n    #replace_with_number=\"\",\n    #replace_with_digit=\"\",\n    replace_with_currency_symbol=\"\",\n    replace_with_punct=\" \",\n    lang=\"en\",)\nprint(new_txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:06.932589Z","iopub.execute_input":"2023-03-08T10:39:06.933067Z","iopub.status.idle":"2023-03-08T10:39:07.429882Z","shell.execute_reply.started":"2023-03-08T10:39:06.933018Z","shell.execute_reply":"2023-03-08T10:39:07.427772Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"ahmed ramzy fathy saleh                                                   details:\nmachine learning engineer on cloud                                                       phone: <phone>\n                                                                                        date of birth: 8/9/200\n                                                                                                                military service: executed\n                                                                                                   mail:ahmed.ramzy.2000.22@gmail.com\nobjectives:\nto get a challenging career that can help me improve my skills & i could express myself through it, to gain more information & more skills in my professional life so i could be useful more & more for a such a successful organization.\nemployment history:\nmachine learning engineer on the cloud at nti, nasr city   \nnov 2022-jan 2023. \neducation:\ngraduated from october 6 university faculty of computer science department of information system (june 2022) \n* cumulative grade: very good \n* graduated project: online pizza ordering system \ntraining &courses: \n* introduction to the network (ccna)                                                  - machine learning on cloud\t\n* egypt fwd. web development challenger track (aug 2021)          -natural language processing \n* egypt fwd. data analytics                                                                    -freelancing skills (12hrs)\n* linked in certificate of data analytics on (july 03, 2022)                  - project management (36hrs)\n* certification of (hcia, 5g) ( jan 07, 2024) \t                         \t       -soft skills (30hrs)\n* hcia big data \n* fundamental of data base mahartech.gov \t        \n* oracle sql database                                                                  \n*introduction to data center (36hr)                                              -\t\n*programming with python (pcap):\n*cloud computing concepts\n-aws core services.\n-aws security.\n-aws architecting.\n-aws cloud economics.\nskills: \n* ms office (word, powerpoint, excel).\n*fast learner\n* adaptability\n*ability to work on a team\n*excellent communication skills\n*creative thinking\nreferences\nupon your request either in soft or hard copy for graduation certificate, military, courses, and other certificates\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n# as per recommendation from @freylis, compile once only\nCLEANR = re.compile('<.*?>') \ndef cleanhtml(new_txt):\n  cleantext = re.sub(CLEANR, '', new_txt)\n  return cleantext\ntext = cleanhtml(new_txt)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:07.431415Z","iopub.execute_input":"2023-03-08T10:39:07.431887Z","iopub.status.idle":"2023-03-08T10:39:07.438070Z","shell.execute_reply.started":"2023-03-08T10:39:07.431849Z","shell.execute_reply":"2023-03-08T10:39:07.436895Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"len(text.split())","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:07.439714Z","iopub.execute_input":"2023-03-08T10:39:07.440410Z","iopub.status.idle":"2023-03-08T10:39:07.452170Z","shell.execute_reply.started":"2023-03-08T10:39:07.440369Z","shell.execute_reply":"2023-03-08T10:39:07.451056Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"244"},"metadata":{}}]},{"cell_type":"code","source":"## prediction using pytorch\nfrom itertools import groupby\n\ndef inferance(text=text, model=model):\n        MAX_LEN = 512\n        id2label = {0:'O', 1:'B-SKILL', 2:'I-SKILL', 3:\"O-SKILL\"}\n        sentence = text #\"i usualy used bootstrap for web tasks and jupyter as ide for python, server sql database\"\n\n        inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n\n        # move to gpu\n        model = model.to(device)\n        ids = inputs[\"input_ids\"].to(device)\n        mask = inputs[\"attention_mask\"].to(device)\n        # forward pass\n        outputs = model(ids, mask)\n        logits = outputs[0]\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        score_softmax = torch.softmax(active_logits, axis=1)  ## calculate softmax for predection value\n        score = torch.max(score_softmax, axis=1)              ## get max output\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n        #score = torch.max(active_logits, axis=1)\n        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n        token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n        wp_preds = list(zip(tokens, token_predictions, score[0])) # list of tuples. Each tuple = (wordpiece, prediction)\n        #yhat_classes = np.where(flattened_predictions.cpu().numpy() > 0.5, 1, 0).squeeze().item()\n\n        word_level_predictions = []\n        for pair in wp_preds:\n          if (pair[0].startswith(\"##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n            # skip prediction\n            continue\n          else:\n            word_level_predictions.append([pair[1], pair[2].item()])\n\n        # we join tokens, if they are not special ones\n        str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n        #print(f\"sentnce length:- {len(str_rep.split())}\", f\"[{str_rep}]\", sep=\"\\n\")\n        #print(f\"predicted_label len:- {len(word_level_predictions)}\", word_level_predictions, sep=\"\\n\")\n        f_ls = list(zip(str_rep.split(), word_level_predictions))\n        skills = [( x[0], x[1][0], x[1][1]) for x in f_ls if x[1][0] == \"B-SKILL\" or x[1][0] == \"I-SKILL\" or x[1][0] == \"O-SKILL\"]\n        #print(yhat_classes)\n        skills = [next(g) for _, g in groupby(skills, key=lambda x:x[0])]\n\n        #print(skills, end=\"\\n\")\n        return skills, tokens","metadata":{"id":"619Bg_mWUdIc","outputId":"d28431cc-db40-4e89-e87b-fd9429bbef91","execution":{"iopub.status.busy":"2023-03-08T10:39:07.456703Z","iopub.execute_input":"2023-03-08T10:39:07.457037Z","iopub.status.idle":"2023-03-08T10:39:07.471444Z","shell.execute_reply.started":"2023-03-08T10:39:07.456980Z","shell.execute_reply":"2023-03-08T10:39:07.470273Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# split resume in sentences\nskills = []\ntokens = []\nfor sub_tex in text.split(\".\"):\n    skill_line, token_line = inferance(text=sub_tex)\n    skills.extend(skill_line)\n    tokens.extend(token_line)\nprint(skills)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:07.473032Z","iopub.execute_input":"2023-03-08T10:39:07.473535Z","iopub.status.idle":"2023-03-08T10:39:08.152534Z","shell.execute_reply.started":"2023-03-08T10:39:07.473475Z","shell.execute_reply":"2023-03-08T10:39:08.151487Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"[('machine', 'B-SKILL', 0.9999083280563354), ('learning', 'I-SKILL', 0.9998332262039185), ('machine', 'B-SKILL', 0.9999176263809204), ('learning', 'I-SKILL', 0.9998291730880737), ('computer', 'B-SKILL', 0.999924898147583), ('science', 'I-SKILL', 0.9995020627975464), ('machine', 'B-SKILL', 0.9999222755432129), ('learning', 'I-SKILL', 0.9998760223388672), ('natural', 'B-SKILL', 0.9993398785591125), ('language', 'I-SKILL', 0.996733546257019), ('processing', 'O-SKILL', 0.9983715415000916), ('analytics', 'B-SKILL', 0.999797523021698), ('certificate', 'B-SKILL', 0.9999103546142578), ('analytics', 'B-SKILL', 0.9998558759689331), ('project', 'B-SKILL', 0.9999485015869141), ('management', 'I-SKILL', 0.9999059438705444), ('big', 'B-SKILL', 0.9999476671218872), ('data', 'I-SKILL', 0.9998098015785217), ('oracle', 'B-SKILL', 0.9999568462371826), ('sql', 'I-SKILL', 0.9997723698616028), ('database', 'B-SKILL', 0.9998849630355835), ('data', 'B-SKILL', 0.9997982382774353), ('center', 'I-SKILL', 0.9998052716255188), ('python', 'B-SKILL', 0.9999485015869141), ('aws', 'B-SKILL', 0.999944806098938), ('aws', 'B-SKILL', 0.99992835521698), ('aws', 'B-SKILL', 0.9999475479125977), ('aws', 'B-SKILL', 0.9999375343322754), ('certificate', 'B-SKILL', 0.9998416900634766)]\n","output_type":"stream"}]},{"cell_type":"code","source":"import itertools\ndef join_tokens(tokens=tokens):\n    res = ''\n    if tokens:\n        res = tokens[0]\n        for token in tokens[1:]:\n            if not (token.isalpha() and res[-1].isalpha()):\n                res += token  # punctuation\n            else:\n                res += ' ' + token  # regular word\n    return res\ndef collapse(skills):\n    # List with the result\n    ner_result = skills\n    collapsed_result = []\n\n\n    current_entity_tokens = []\n    current_entity = None\n\n    # Iterate over the tagged tokens\n    for token, tag, score in ner_result:\n\n        if tag.startswith(\"B-\"):\n            # ... if we have a previous entity in the buffer, store it in the result list\n            if current_entity is not None:\n                collapsed_result.append([join_tokens(current_entity_tokens), current_entity])\n\n            current_entity = tag[2:]\n            # The new entity has so far only one token\n            current_entity_tokens = [token]\n\n        # If the entity continues ...\n        elif current_entity_tokens!= None and tag == \"I-\" + str(current_entity):\n            # Just add the token buffer\n            current_entity_tokens.append(token)\n        elif current_entity_tokens!= None and tag == \"O-\" + str(current_entity):\n            # Just add the token buffer\n            current_entity_tokens.append(token)\n        else:\n            collapsed_result.append([join_tokens(current_entity_tokens), current_entity])\n            collapsed_result.append([token,tag[2:]])\n\n            current_entity_tokens = []\n            current_entity = None\n\n            pass\n\n    # The last entity is still in the buffer, so add it to the result\n    # ... but only if there were some entity at all\n    if current_entity is not None:\n        collapsed_result.append([join_tokens(current_entity_tokens), current_entity])\n        collapsed_result = sorted(collapsed_result)\n        collapsed_result = list(k for k, _ in groupby(collapsed_result))\n\n    return collapsed_result\ncollapsed_result = collapse(skills)\ncollapsed_result =  dict(collapsed_result)\nfor entity in collapsed_result.keys():\n    for skill in skills:\n        if skill[0] in entity.split()[-1]:\n            collapsed_result[entity] = \"SKILL\" +\", score: \"+ \"{:.2f}\".format(skill[2])\nprint(collapsed_result)\n            # collapsed_result","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:08.154657Z","iopub.execute_input":"2023-03-08T10:39:08.155411Z","iopub.status.idle":"2023-03-08T10:39:08.169328Z","shell.execute_reply.started":"2023-03-08T10:39:08.155369Z","shell.execute_reply":"2023-03-08T10:39:08.168161Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"{'analytics': 'SKILL, score: 1.00', 'aws': 'SKILL, score: 1.00', 'big data': 'SKILL, score: 1.00', 'certificate': 'SKILL, score: 1.00', 'computer science': 'SKILL, score: 1.00', 'data center': 'SKILL, score: 1.00', 'database': 'SKILL, score: 1.00', 'machine learning': 'SKILL, score: 1.00', 'natural language processing': 'SKILL, score: 1.00', 'oracle sql': 'SKILL, score: 1.00', 'project management': 'SKILL, score: 1.00', 'python': 'SKILL, score: 1.00'}\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nfrom transformers import pipeline\ntoken = []\nfor sub_tex in text.split(\".\"):\n    pipe = pipeline(task=\"ner\",model=model.to('cpu'),tokenizer=tokenizer, aggregation_strategy=\"simple\")\n    #r\"I usualy used bootstrap for web tasks and jupyter as ide for python, server sql\"\n    token.extend(pipe(sub_tex))\n#print(token)\ntoken_md = token\nentities = []\nfor row, entity in enumerate(token_md):\n    if re.search(\"#\", entity[\"word\"]) != None:\n        word = re.sub(\"#\", \"\", entity[\"word\"])\n        #word = entity[\"word\"]\n        entities[-1][\"word\"] = entities[-1][\"word\"] + word\n        #token_md.remove(entity)\n    else:\n        entities.append(entity)\nprint(entities)","metadata":{"id":"LW0lWKD_UsxT","outputId":"4792d171-f23d-4557-bbe8-a7981f482225","execution":{"iopub.status.busy":"2023-03-08T10:39:08.171065Z","iopub.execute_input":"2023-03-08T10:39:08.171434Z","iopub.status.idle":"2023-03-08T10:39:21.392002Z","shell.execute_reply.started":"2023-03-08T10:39:08.171397Z","shell.execute_reply":"2023-03-08T10:39:21.390812Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[{'entity_group': 'SKILL', 'score': 0.9998708, 'word': 'machine learning', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9998734, 'word': 'machine learning', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99971354, 'word': 'computer science', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99989915, 'word': 'machine learning', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99803674, 'word': 'natural language', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99837154, 'word': 'processing', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9997975, 'word': 'analytics', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9999105, 'word': 'certificate', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9998559, 'word': 'analytics', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9999273, 'word': 'project management', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99987876, 'word': 'big data', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9998646, 'word': 'oracle sql', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99988496, 'word': 'database', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9998017, 'word': 'data center', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9999485, 'word': 'python', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9999448, 'word': 'aws', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99992836, 'word': 'aws', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99994755, 'word': 'aws', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.99993753, 'word': 'aws', 'start': None, 'end': None}, {'entity_group': 'SKILL', 'score': 0.9998417, 'word': 'certificate', 'start': None, 'end': None}]\nCPU times: user 5.91 s, sys: 1.4 s, total: 7.31 s\nWall time: 13.2 s\n","output_type":"stream"}]},{"cell_type":"code","source":"## saving bert model to run on local machine\nimport os\n\ndirectory = \"/kaggle/working//model\"\n\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# save vocabulary of the tokenizer\ntokenizer.save_vocabulary(directory)\n# save the model weights and its configuration file\nmodel.save_pretrained(directory)\nprint('All files saved')\nprint('This tutorial is completed')","metadata":{"execution":{"iopub.status.busy":"2023-03-08T10:39:21.393802Z","iopub.execute_input":"2023-03-08T10:39:21.394648Z","iopub.status.idle":"2023-03-08T10:39:22.311349Z","shell.execute_reply.started":"2023-03-08T10:39:21.394605Z","shell.execute_reply":"2023-03-08T10:39:22.309844Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"All files saved\nThis tutorial is completed\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}